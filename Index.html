<!DOCTYPE html>
<html lang="en">
<head>

  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Voice-Powered Chat with PolyCam</title>

  <style>

    html, body {
      margin: 0;
      padding: 0;
      width: 100%;
      height: 100%;
      /* Prevent horizontal scrollbar */
      overflow-x: hidden;
      overflow-y: hidden;
    }

    /* Container holds the iframe and fullscreen button */
    #container {
      position: relative;
      width: 100%;
      height: 100%;
    }

    /* Fullscreen button styling - positioned bottom-right */
    #fullscreenBtn {
      position: absolute;
      bottom: 10px;
      right: 15px;
      z-index: 100;
      background-color: rgba(255, 255, 255, 0.185);
      color: #fff;
      border: none;
      padding: 10px;
      cursor: pointer;
      font-size: 0px;
      border-radius: 4px;
    }

    /* Make the iframe fill the container */
    iframe {
      width: 100%;
      height: 100%;
      border: none;
      z-index: 1; 
      /* æ–°å¢çš„é»‘ç™½æ»¤é•œ */
      animation: artFilterCycle 35s infinite ease-in-out;
    }
    
    @keyframes videoBrightness {
      0% {
        filter: opacity(0%);
      }
      50% {
        filter: opacity(100%);
      }
      100% {
        filter: opacity(0%);
      }
    }

    @keyframes artFilterCycle {
      0% {
        filter: grayscale(0%) sepia(0%) contrast(100%) brightness(100%);
      }
      25% {
        filter: grayscale(80%) sepia(30%) contrast(110%) brightness(90%);
      }
      50% {
        filter: grayscale(100%) sepia(60%) contrast(120%) brightness(80%);
      }
      75% {
        filter: grayscale(60%) sepia(40%) contrast(115%) brightness(85%);
      }
      100% {
        filter: grayscale(0%) sepia(0%) contrast(100%) brightness(100%);
      }
    }

    /* æ–°çš„æ–‡å­—è§†é¢‘å®¹å™¨æ ·å¼ */
    #videoTextContainer {
      position: absolute;
      top: 0px;
      right: -200px;
      width: 900px;
      z-index: 10;
      pointer-events: none;
      /* ä½¿ç”¨æ··åˆæ¨¡å¼æ»¤é™¤é»‘è‰²èƒŒæ™¯ */
      mix-blend-mode: screen;
    }

    #textVideo {
      width: 100%;
      height: 100%;
      object-fit: contain;
    }

    /* Default positioning for the video container */
    #videoContainer {
      position: absolute;
      left: 100px;
      bottom: 100px;
      width: 480px;
      height: 200px;
      overflow: hidden;
      outline: 0px solid #ffffff; 
      outline-offset: 0;
      box-shadow: 0 0 10px 10px rgba(0, 0, 0, 0.526);
    }
    
    /* When the viewport is short, force the video container to be at least 100px from the top */
    @media (max-height: 420px) {
      #videoContainer {
        top: 100px;
      }
    }
    
    /* Video styling */
    #overlayVideo {
      width: 100%;
      height: 100%;
      object-fit: cover;
      object-position: center center;
      display: block;
    }

    /* æ–°å¢å¯¹è¯æ¡†æ ·å¼ */
    .chat-container {
      position: absolute;
      left: 90px;
      bottom: 320px; /* è°ƒæ•´ä½ç½®é¿å…ä¸è§†é¢‘å®¹å™¨é‡å  */
      width: 490px;
      max-height: 300px;
      background: rgba(0, 0, 0, 0);
      color: white;
      overflow: hidden;
      font-family: system-ui, -apple-system, sans-serif; /* æ–°å¢ */
    }

    .chat-history {
      height: 180px;
      overflow-y: auto;
      text-align: left;
    }

    .chat-history::-webkit-scrollbar {
      width: 8px;
      height: 8px;
      background: transparent;
    }

    .chat-history::-webkit-scrollbar-thumb {
      background: rgba(255, 255, 255, 0.3);
      border-radius: 4px;
      transition: background 0.3s;
    }

    /* æ‚¬åœçŠ¶æ€ */
    .chat-history::-webkit-scrollbar-thumb:hover {
      background: rgba(255, 255, 255, 0.6); /* æ›´æ˜æ˜¾çš„ç™½è‰² */
    }

    #transcription-status {
      color: #aaa;
      font-style: italic;
      margin-bottom: 10px;
    }

    #toggle-mic-btn {
      position: absolute;
      left: 10px;
      bottom: 0px;
      z-index: 10;
      background-color:rgba(255, 255, 255, 0.6);
      color: #fff;
      border: none;
      padding: 8px 16px;
      border-radius: 0px;
      font-size: 12px;
      cursor: pointer;
    }

  </style>

</head>

<body>

  <div id="container">

    <button id="fullscreenBtn">
      <img src="./fullscreen-svgrepo-com.svg" style="width: 30px; height:30px;">
    </button>

    <!-- Polycam Embed Code -->
    <iframe src="https://poly.cam/capture/fca35082-b2b5-45ca-b137-b8845bbc852a/embed" 
            allow="camera; microphone; fullscreen; accelerometer; gyroscope; magnetometer; vr; xr-spatial-tracking">
    </iframe>

    <div id="videoTextContainer">
      <video id="textVideo" src="TDMovieOut.1.mp4" autoplay muted loop preload="auto" playsinline></video>
    </div>

    <!-- Overlay Video -->
    <div id="videoContainer">
      <video id="overlayVideo" src="./1.0.mp4" autoplay muted loop></video>
    </div>

    <!-- æ–°å¢èŠå¤©å®¹å™¨ -->
    <div class="chat-container">
      <div class="chat-history" id="chat-history"></div>
      <!-- New voice interface elements -->
      <div id="transcription-status">
        <!-- Real-time transcription will appear here -->
      </div>
      <button id="toggle-mic-btn">
        ğŸ™ï¸ Start Listening
      </button>
    </div>
  </div>

  <script>
    const fullscreenBtn = document.getElementById('fullscreenBtn');
    const container = document.getElementById('container');

    // ======== æ–°å¢åŠ¨ç”»æ§åˆ¶å™¨ä»£ç  ========
    function initVideoAnimation() {
      const video = document.getElementById('textVideo');
      const container = document.getElementById('videoTextContainer');

      const setAnimation = () => {
        if(video.duration > 0) {
          const duration = video.duration;
          
          // å¼ºåˆ¶é‡ç»˜æŠ€å·§
          container.style.animation = 'none';
          void container.offsetHeight; // è§¦å‘ reflow
          
          container.style.animation = 
            `videoBrightness ${duration}s infinite ease-in-out`;
        }
      };

      // äº‹ä»¶ç›‘å¬ä¼˜åŒ–ç‰ˆæœ¬
      const events = ['loadedmetadata', 'canplay', 'playing'];
      events.forEach(e => video.addEventListener(e, setAnimation));

      // Safari å…¼å®¹æ–¹æ¡ˆ
      let loadAttempts = 0;
      const safariCheck = setInterval(() => {
        if(video.readyState > 0 || loadAttempts++ > 5) {
          setAnimation();
          clearInterval(safariCheck);
        }
      }, 500);
    }

    // åˆå§‹åŒ–æ—¶æœºæ§åˆ¶
    if(document.readyState === 'complete') {
      initVideoAnimation();
    } else {
      window.addEventListener('load', initVideoAnimation);
      document.addEventListener('DOMContentLoaded', initVideoAnimation);
    }

    fullscreenBtn.addEventListener('click', () => {
      if (!document.fullscreenElement) {
        container.requestFullscreen().catch(err => {
          console.error(`Error attempting to enable full-screen mode: ${err.message}`);
        });
      } else {
        document.exitFullscreen();
      }
    });

  </script>

  <!-- Include Socket.io client library for WebSocket communication -->
  <script src="/socket.io/socket.io.js"></script>

  <script>

    // Check browser support for Web Speech API
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

    if (!SpeechRecognition) {
      alert("Sorry, your browser doesn't support speech recognition. Please use Chrome/Edge, etc.");
    } else {
      const recognition = new SpeechRecognition();
      recognition.continuous = true;       // keep listening until stopped
      recognition.interimResults = true;   // show real-time interim results
      recognition.lang = "en-US";          // language for recognition

      const socket = io();  // connect to Node.js server via WebSocket
      const chatHistory = document.getElementById('chat-history');
      const statusDiv   = document.getElementById('transcription-status');
      const micBtn      = document.getElementById('toggle-mic-btn');
      let listening = false;  // to track if mic is active

      // Helper to append messages to chat history
      function appendMessage(sender, text) {
        const msgDiv = document.createElement('div');
        msgDiv.className = sender === 'user' ? 'user-message' : 'assistant-message';
        msgDiv.textContent = text;
        chatHistory.appendChild(msgDiv);
        chatHistory.scrollTop = chatHistory.scrollHeight;  // auto-scroll to bottom
      }

      micBtn.addEventListener('click', async () => {
        try {
          if (!listening) {
            // æ˜¾å¼è¯·æ±‚éº¦å…‹é£æƒé™
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            stream.getTracks().forEach(track => track.stop());
      
            recognition.start();
            micBtn.textContent = "ğŸ”´ Stop Listening";
            statusDiv.textContent = "Listening...";
            listening = true;
          } else {
            recognition.stop();
            micBtn.textContent = "ğŸ™ï¸ Start Listening";
            statusDiv.textContent = "";
            listening = false;
          }
        } catch (err) {
          console.error('éº¦å…‹é£è®¿é—®é”™è¯¯:', err);
          statusDiv.textContent = "è¯·å…è®¸éº¦å…‹é£è®¿é—®";
          micBtn.textContent = "ğŸ™ï¸ Start Listening";
          listening = false;
        }
      });

      // å¢åŠ è¯­éŸ³è¯†åˆ«é”™è¯¯å¤„ç†
      recognition.onerror = (event) => {
        console.error('è¯†åˆ«é”™è¯¯:', event.error);
        if (event.error === 'not-allowed') {
          statusDiv.textContent = 'è¯·å…è®¸éº¦å…‹é£è®¿é—®';
        }
        listening = false;
        micBtn.textContent = "ğŸ™ï¸ Start Listening";
      };

      // Process speech recognition results in real time
      recognition.onresult = (event) => {
        let interimTranscript = "";
        let finalTranscript = "";
        // Aggregate interim and final results from the event
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const result = event.results[i];
          if (result.isFinal) {
            finalTranscript += result[0].transcript;
          } else {
            interimTranscript += result[0].transcript;
          }
        }
        // Display interim transcript (partial speech) if any
        if (interimTranscript) {
          statusDiv.textContent = "Listening: " + interimTranscript;
        }
        // When a final transcript is available, send it to server and display it
        if (finalTranscript) {
          finalTranscript = finalTranscript.trim();
          if (finalTranscript.length > 0) {
            // Show the finalized user speech in chat history
            appendMessage('user', finalTranscript);
            statusDiv.textContent = "";  // clear interim display
            // Emit the transcribed message to the backend for OpenAI processing
            socket.emit('transcript', finalTranscript);
          }
        }
      };

      // Restart recognition automatically if it stops (for continuous listening)
      recognition.onend = () => {
        if (listening) {
          // If the mic is still supposed to be on, restart listening (user paused speaking)
          recognition.start();
        }
      };

      recognition.onerror = (err) => {
        console.error("Speech recognition error:", err);
        // If an error occurs but mic is still on, try restarting after a short delay
        if (listening) {
          statusDiv.textContent = "âš ï¸ Mic error, restarting...";
          setTimeout(() => recognition.start(), 500);
        }
      };

      // Receive AI assistant responses from the server and display them
      socket.on('assistantResponse', (message) => {
        appendMessage('assistant', message);
      });
    }

  </script>

</body>

</html>