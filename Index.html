<!DOCTYPE html>
<html lang="en">
<head>

  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Voice-Powered Chat with PolyCam</title>

  <style>

    html, body {
      margin: 0;
      padding: 0;
      width: 100%;
      height: 100%;
      /* Prevent horizontal scrollbar */
      overflow-x: hidden;
      overflow-y: hidden;
    }

    /* Container holds the iframe and fullscreen button */
    #container {
      position: relative;
      width: 100%;
      height: 100%;
    }

    /* Fullscreen button styling - positioned bottom-right */
    #fullscreenBtn {
      position: absolute;
      bottom: 10px;
      right: 18px;
      z-index: 100;
      background-color: rgba(255, 255, 255, 0.185);
      color: #fff;
      border: none;
      padding: 5px;
      cursor: pointer;
      font-size: 0px;
      border-radius: 4px;
    }

    /* Make the iframe fill the container */
    iframe {
      width: 100%;
      height: 100%;
      border: none;
      z-index: 1; 
      /* æ–°å¢çš„é»‘ç™½æ»¤é•œ */
      animation: artFilterCycle 35s infinite ease-in-out;
    }
    
    @keyframes videoBrightness {
      0% {
        filter: opacity(0%);
      }
      50% {
        filter: opacity(100%);
      }
      100% {
        filter: opacity(0%);
      }
    }

    @keyframes artFilterCycle {
      0% {
        filter: grayscale(0%) sepia(0%) contrast(100%) brightness(100%);
      }
      25% {
        filter: grayscale(80%) sepia(30%) contrast(110%) brightness(90%);
      }
      50% {
        filter: grayscale(100%) sepia(60%) contrast(120%) brightness(80%);
      }
      75% {
        filter: grayscale(60%) sepia(40%) contrast(115%) brightness(85%);
      }
      100% {
        filter: grayscale(0%) sepia(0%) contrast(100%) brightness(100%);
      }
    }

    /* æ–°çš„æ–‡å­—è§†é¢‘å®¹å™¨æ ·å¼ */
    #videoTextContainer {
      position: absolute;
      top: 0px;
      right: -200px;
      width: 900px;
      z-index: 10;
      pointer-events: none;
      /* ä½¿ç”¨æ··åˆæ¨¡å¼æ»¤é™¤é»‘è‰²èƒŒæ™¯ */
      mix-blend-mode: screen;
    }

    #textVideo {
      width: 100%;
      height: 100%;
      object-fit: contain;
    }

    /* Default positioning for the video container */
    #videoContainer {
      position: absolute;
      left: 100px;
      bottom: 100px;
      width: 480px;
      height: 200px;
      overflow: hidden;
      outline: 0px solid #ffffff; 
      outline-offset: 0;
      box-shadow: 0 0 10px 10px rgba(0, 0, 0, 0.526);
    }
    
    /* When the viewport is short, force the video container to be at least 100px from the top */
    @media (max-height: 420px) {
      #videoContainer {
        top: 100px;
      }
    }
    
    /* Video styling */
    #overlayVideo {
      width: 100%;
      height: 100%;
      object-fit: cover;
      object-position: center center;
      display: block;
    }

    /* ä¿®æ”¹åçš„èŠå¤©å®¹å™¨æ ·å¼ */
    .chat-container {
        position: absolute;
        left: 100px;
        bottom: 320px;
        width: 480px;
        height: 300px; /* å›ºå®šé«˜åº¦æ›¿ä»£max-height */
        background: rgba(0, 0, 0, 0);
        color: white;
        overflow: hidden;
        font-family: system-ui, -apple-system, sans-serif;
        display: flex; /* æ–°å¢flexå¸ƒå±€ */
        flex-direction: column; /* å‚ç›´æ’åˆ— */
    }

    .chat-history {
      height:288px;
      overflow-y: auto;
      text-align: left;
      padding-bottom: 45px;
      font-size: 10.5px;
    }

    .chat-history::-webkit-scrollbar {
      width: 8px;
      height: 8px;
      background: transparent;
    }

    .chat-history::-webkit-scrollbar-thumb {
      background: rgba(255, 255, 255, 0.3);
      border-radius: 4px;
      transition: box-shadow 0.3s ease; 
    }

    /* æ‚¬åœçŠ¶æ€ */
    .chat-history::-webkit-scrollbar-thumb:hover {
      background: rgba(255, 255, 255, 0.6); /* æ›´æ˜æ˜¾çš„ç™½è‰² */
    }

    #transcription-status {
      color: #aaa;
      font-style: italic;
      margin-bottom: 10px;
      font-size: 10.5px;
    }

    /* ä¿®æ”¹åçš„æŒ‰é’®å®šä½ */
    #toggle-mic-btn {
      position: relative; /* æ”¹ä¸ºç›¸å¯¹å®šä½ */
      left: 0;
      bottom: 0;
      margin-top: auto; /* è‡ªåŠ¨æ¨åˆ°å®¹å™¨åº•éƒ¨ */
      width: min-content;
      min-width: 120px; /* æœ€å°å®½åº¦ä¿è¯æ–‡å­—ä¸æ¢è¡Œ */
      white-space: nowrap; /* é˜²æ­¢æ–‡å­—æ¢è¡Œ */
      z-index: 10;
      background-color: rgba(255, 255, 255, 0.6);
      color: #fff;
      border: none;
      padding: 8px 16px;
      border-radius: 0px;
      font-size: 12px;
      cursor: pointer;
    }

    #liveStreamContainer {
      position: absolute;
      width: 750px;
      height: 750px;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      z-index: 20; /* ç¡®ä¿å®ƒä½äºå…¶ä»–å…ƒç´ ä¹‹ä¸Š */
    }

    /* Live stream video styling */
    .streamVideo {
      width: 100%;
      height: 100%;
      border-radius: 50%;
      overflow: hidden;
      background: #ffffff;
      z-index: 30;
    }

  </style>

</head>

<body>

  <div id="container">

    <button id="fullscreenBtn">
      <img src="./fullscreen-svgrepo-com.svg" style="width: 30px; height:30px;">
    </button>

    <!-- Polycam Embed Code -->
    <iframe src="https://poly.cam/capture/fca35082-b2b5-45ca-b137-b8845bbc852a/embed" 
            allow="camera; microphone; fullscreen; accelerometer; gyroscope; magnetometer; vr; xr-spatial-tracking">
    </iframe>

    <div id="videoTextContainer">
      <video id="textVideo" src="TDMovieOut.1.mp4" autoplay muted loop preload="auto" playsinline></video>
    </div>

    <!-- Overlay Video -->
    <div id="videoContainer">
      <video id="overlayVideo" src="./1.0.mp4" autoplay muted loop></video>
    </div>

    <!-- æ–°å¢èŠå¤©å®¹å™¨ -->
    <div class="chat-container">
      <div class="chat-history" id="chat-history"></div>
      <!-- New voice interface elements -->
      <div id="transcription-status">
        <!-- Real-time transcription will appear here -->
      </div>
      <button id="toggle-mic-btn">
        ğŸ™ï¸ Start Listening
      </button>
    </div>

    <!-- Insert the live stream video element where desired -->
    <div id="liveStreamContainer">
      <video class="streamVideo" autoplay muted playsinline></video>
    </div>

  </div>

  <script>
    // åœ¨é¡µé¢åŠ è½½åå¼ºåˆ¶è®¾ç½®å¾ªç¯
    window.addEventListener('load', () => {
      const videos = document.querySelectorAll('video');
      videos.forEach(video => {
        video.loop = true; // æ˜¾å¼è®¾ç½®å¾ªç¯
        
        // å…¼å®¹æ€§å¤„ç†
        video.addEventListener('ended', () => {
          video.currentTime = 0;
          video.play();
        });
      });
    });
  </script>

  <script>
    const fullscreenBtn = document.getElementById('fullscreenBtn');
    const container = document.getElementById('container');

    // ======== æ–°å¢åŠ¨ç”»æ§åˆ¶å™¨ä»£ç  ========
    function initVideoAnimation() {
      const video = document.getElementById('textVideo');
      const container = document.getElementById('videoTextContainer');

      const setAnimation = () => {
        if(video.duration > 0) {
          const duration = video.duration;
          
          // å¼ºåˆ¶é‡ç»˜æŠ€å·§
          container.style.animation = 'none';
          void container.offsetHeight; // è§¦å‘ reflow
          
          container.style.animation = 
            `videoBrightness ${duration}s infinite ease-in-out`;
        }
      };

      // äº‹ä»¶ç›‘å¬ä¼˜åŒ–ç‰ˆæœ¬
      const events = ['loadedmetadata', 'canplay', 'playing'];
      events.forEach(e => video.addEventListener(e, setAnimation));

      // Safari å…¼å®¹æ–¹æ¡ˆ
      let loadAttempts = 0;
      const safariCheck = setInterval(() => {
        if(video.readyState > 0 || loadAttempts++ > 5) {
          setAnimation();
          clearInterval(safariCheck);
        }
      }, 500);
    }

    // åˆå§‹åŒ–æ—¶æœºæ§åˆ¶
    if(document.readyState === 'complete') {
      initVideoAnimation();
    } else {
      window.addEventListener('load', initVideoAnimation);
      document.addEventListener('DOMContentLoaded', initVideoAnimation);
    }

    fullscreenBtn.addEventListener('click', () => {
      if (!document.fullscreenElement) {
        container.requestFullscreen().catch(err => {
          console.error(`Error attempting to enable full-screen mode: ${err.message}`);
        });
      } else {
        document.exitFullscreen();
      }
    });

  </script>

  <!-- Include Socket.io client library for WebSocket communication -->
  <script src="/socket.io/socket.io.js"></script>

  <script>
    // ========== é€šä¿¡æ¨¡å— ==========
    let lastCheckTime = Date.now();
    let lastCharCount = 0;
    let accumulatedBuffer = "";
    const CHAR_THRESHOLD = 100;
    const CHECK_INTERVAL = 90000;
    
    // ========== å…¨å±€é€šä¿¡æ¨¡å— ==========
    let socket; // å…¨å±€å£°æ˜

    window.addEventListener('load', () => {
      // åˆå§‹åŒ– Socket è¿æ¥ï¼ˆç¡®ä¿æœåŠ¡å™¨åœ°å€æ­£ç¡®ï¼‰
      socket = io('http://localhost:3000'); 

      // æ·»åŠ è¿æ¥çŠ¶æ€ç›‘å¬
      socket.on('connect', () => {
        console.log('Socket connected:', socket.id);
        socket.on('assistantResponse', (message) => {
          appendMessage('assistant', message);
        });
      });
      

      socket.on('disconnect', () => {
        console.log('Socket disconnected');
      });
    });

    // åˆå§‹åŒ–å­—ç¬¦ç»Ÿè®¡
    function initCharCounter() {
      const chatHistory = document.getElementById('chat-history');
      if (!chatHistory) return;
      lastCharCount = chatHistory.textContent.length;
    }

    // å®šæ—¶æ£€æµ‹å‡½æ•°
    function checkChatActivity() {
      const chatHistory = document.getElementById('chat-history');
      if (!chatHistory) return;
      const currentContent = chatHistory.textContent;
      const newChars = currentContent.slice(lastCharCount);
      accumulatedBuffer += newChars;
      lastCharCount = currentContent.length; // å…³é”®ä¿®å¤ï¼šå¿…é¡»æ›´æ–°
      if (accumulatedBuffer.length >= CHAR_THRESHOLD) {
        console.log("ğŸš€ è§¦å‘ä¼ è¾“:", accumulatedBuffer);
        socket.emit('td-generate', {
          timestamp: Date.now(),
          text: accumulatedBuffer
        });
        accumulatedBuffer = "";
      } else {
        console.log("â³ ç´¯ç§¯è¿›åº¦:", 
          `${accumulatedBuffer.length}/${CHAR_THRESHOLD}`,
          "æœ€æ–°å†…å®¹:", newChars
        );
      }
    }

    // å®šæ—¶å™¨ç®¡ç†
    let checkTimer;
    function startCheckTimer() {
      checkTimer = setInterval(checkChatActivity, CHECK_INTERVAL);
      checkChatActivity(); // ç«‹å³æ‰§è¡Œä¸€æ¬¡
    }

    // é¡µé¢ç”Ÿå‘½å‘¨æœŸç®¡ç†
    window.addEventListener('load', () => {
      initCharCounter();
      startCheckTimer();
    });

    document.addEventListener('visibilitychange', () => {
      if (document.hidden) {
        clearInterval(checkTimer);
      } else {
        startCheckTimer();
      }
    });

  </script>

  <script>

    // Check browser support for Web Speech API
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    const debounce = (func, delay) => {
      let timeoutId;
      return (...args) => {
        clearTimeout(timeoutId);
        timeoutId = setTimeout(() => func(...args), delay);
      };
    };

    if (!SpeechRecognition) {
      alert("Sorry, your browser doesn't support speech recognition. Please use Chrome/Edge, etc.");
    } else {
      const recognition = new SpeechRecognition();
      recognition.continuous = true;       // keep listening until stopped
      recognition.interimResults = true;   // show real-time interim results
      recognition.lang = "en-US";          // language for recognition

      
      let interimBuffer = ""; // æ·»åŠ è¿™ä¸€è¡Œ
      const chatHistory = document.getElementById('chat-history');
      const statusDiv   = document.getElementById('transcription-status');
      const micBtn      = document.getElementById('toggle-mic-btn');
      let listening = false;  // to track if mic is active

      // ä¿®æ”¹åçš„appendMessageå‡½æ•°
      function appendMessage(sender, text) {
        const msgDiv = document.createElement('div');
        msgDiv.className = sender === 'user' ? 'user-message' : 'assistant-message';
        msgDiv.textContent = text;
        
        // åœ¨æ·»åŠ å…ƒç´ å‰è·å–å½“å‰æ»šåŠ¨çŠ¶æ€
        const container = chatHistory;
        const wasAtBottom = container.scrollHeight - container.clientHeight <= container.scrollTop + 1;

        chatHistory.appendChild(msgDiv);

        // ä½¿ç”¨åŒRAFç¡®ä¿DOMæ›´æ–°å®Œæˆ
        requestAnimationFrame(() => {
          requestAnimationFrame(() => {
            // å¼ºåˆ¶æ»šåŠ¨åˆ°åº•éƒ¨
            container.scrollTop = container.scrollHeight;
                
            // å¦‚æœå½“å‰ä¸åœ¨åº•éƒ¨ï¼Œæ·»åŠ è§†è§‰æç¤º
            if (!wasAtBottom) {
              container.style.boxShadow = 'inset 0 -5px 10px rgba(255,255,255,0.3)';
              setTimeout(() => {
                container.style.boxShadow = 'none';
              }, 500);
            }
          });
        });
      }

      micBtn.addEventListener('click', async () => {
        try {
          if (!listening) {
            // æ˜¾å¼è¯·æ±‚éº¦å…‹é£æƒé™
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            stream.getTracks().forEach(track => track.stop());
      
            recognition.start();
            micBtn.textContent = "ğŸ”´ Stop Listening";
            statusDiv.textContent = "Listening...";
            listening = true;
          } else {
            recognition.stop();
            micBtn.textContent = "ğŸ™ï¸ Start Listening";
            statusDiv.textContent = "";
            listening = false;
          }
        } catch (err) {
          console.error('éº¦å…‹é£è®¿é—®é”™è¯¯:', err);
          statusDiv.textContent = "è¯·å…è®¸éº¦å…‹é£è®¿é—®";
          micBtn.textContent = "ğŸ™ï¸ Start Listening";
          listening = false;
        }
      });

      // å¢åŠ è¯­éŸ³è¯†åˆ«é”™è¯¯å¤„ç†
      recognition.onerror = (event) => {
        console.error('è¯†åˆ«é”™è¯¯:', event.error);
        if (event.error === 'not-allowed') {
          statusDiv.textContent = 'è¯·å…è®¸éº¦å…‹é£è®¿é—®';
        }
        listening = false;
        micBtn.textContent = "ğŸ™ï¸ Start Listening";
      };

      // ä¿®æ”¹åçš„å˜é‡
      let confirmedSegments = [];  // å­˜å‚¨æ‰€æœ‰å·²ç¡®è®¤çš„æ®µè½
      let currentDraft = "";       // å½“å‰ä¸´æ—¶è‰ç¨¿
      let silenceTimer = null;
      const SILENCE_TIMEOUT = 5000;

      // ç»Ÿä¸€æäº¤å‡½æ•°
      //function commitFinalTranscript() {
        //if (pendingFinal.trim().length > 0) {
          //handleFinalTranscript(pendingFinal);
          //pendingFinal = "";
        //}
        //clearTimeout(silenceTimer);
        //}

      function handleFinalTranscript(rawText) {

        // æ¸…æ´—æ­¥éª¤ï¼š
        let cleanText = rawText
          .replace(/(\b\w+\b)(\s+\1\b)+/gi, "$1") // å»é‡ï¼šå°†é‡å¤çš„å•è¯åˆå¹¶
          .replace(/\s{2,}/g, " ")                // åˆå¹¶å¤šä½™ç©ºæ ¼
          .trim();
        
        // 2. æ™ºèƒ½åˆ†æ®µï¼ˆé˜²æ­¢å¥å­æˆªæ–­ï¼‰
        const sentences = cleanText.match(/[^.!?]+[.!?]*/g) || [];
        cleanText = sentences.join(" ").trim();

        if (cleanText) {
          appendMessage('user', cleanText);
          socket.emit('transcript', cleanText);
          statusDiv.textContent = "å·²æäº¤ âœ“";
        }
      }

      let debugCount = 0;

      recognition.onresult = (event) => {
    
        console.log(`å¤„ç†ç»“æœ #${++debugCount}`, event);

        clearTimeout(silenceTimer); // å¿…é¡»é¦–å…ˆæ¸…é™¤æ—§è®¡æ—¶å™¨

        let hasNewFinal = false;
    
        // å¤„ç†æ‰€æœ‰ç»“æœ
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const result = event.results[i];
          const text = result[0].transcript.trim();
    
          if (result.isFinal) {
            // æœ€ç»ˆç»“æœï¼šå­˜å…¥å†å²æ®µè½
            confirmedSegments.push(text);
            hasNewFinal = true;
          } else {
            // ä¸´æ—¶ç»“æœï¼šæ›´æ–°å½“å‰è‰ç¨¿
            currentDraft = text;
          }
        }

        // æ„å»ºå®Œæ•´å†…å®¹
        let fullText = "";

        if (confirmedSegments.length > 0) {
          fullText = confirmedSegments.join(". ") //+ (currentDraft ? ` ${currentDraft}` : "");
        } else {
          fullText = currentDraft;
        }

        // æ›´æ–°æ˜¾ç¤º
        statusDiv.textContent = hasNewFinal 
          ? `å·²ç¡®è®¤æ®µè½: ${confirmedSegments.join(" | ")}`
          : currentDraft 
            ? `è¾“å…¥ä¸­: ${currentDraft}`
            : "";

        const debouncedHandle = debounce((text) => {
          handleFinalTranscript(text);
          confirmedSegments = [];
          currentDraft = "";
        }, 300); // 300msé˜²æŠ–çª—å£

        silenceTimer = setTimeout(() => debouncedHandle(fullText), 5000);
      };

      // Restart recognition automatically if it stops (for continuous listening)
      recognition.onend = () => {
        if (listening) {
          // If the mic is still supposed to be on, restart listening (user paused speaking)
          recognition.start();
        }
      };

      // éœ€è¦ä¿®æ”¹çš„é”™è¯¯å¤„ç†ä»£ç æ®µ
      recognition.onerror = (err) => {
        console.error("Speech recognition error:", err);
        clearTimeout(silenceTimer);
    
        // æ–°å¢é”™è¯¯ç±»å‹è¿‡æ»¤
        const ignorableErrors = [
          'no-speech',         // æ— è¯­éŸ³è¾“å…¥
          'audio-capture',     // éŸ³é¢‘æ•è·é—®é¢˜
          'network'            // ç½‘ç»œé”™è¯¯
        ];

        if (listening) {
          // å¯¹å¯å¿½ç•¥é”™è¯¯ä¸æ˜¾ç¤ºæç¤º
          if (!ignorableErrors.includes(err.error)) {
            statusDiv.textContent = "âš ï¸ Mic error, restarting...";
          }
        
          // ä¼˜åŒ–é‡å¯ç­–ç•¥
          const retry = () => {
            recognition.stop();  // å…ˆæ­£å¸¸åœæ­¢
            setTimeout(() => {
              recognition.start().catch(error => {
                console.log('é‡å¯å¤±è´¥:', error);
              });
            }, 500);
          };
        
          // æ ¹æ®é”™è¯¯ç±»å‹å¤„ç†
          switch(err.error) {
            case 'no-speech':
              // æ— è¯­éŸ³æ—¶ä¸é‡å¯
              break;
            case 'network':
              retry();
              break;
            default:
              setTimeout(retry, 1000);
          }
        }
      };

      // Receive AI assistant responses from the server and display them
      //socket.on('assistantResponse', (message) => {
        //appendMessage('assistant', message);
      //});
    }

  </script>

<script>
  // Connect to TouchDesignerâ€™s signaling server via WebSocket
  const ws = new WebSocket('ws://localhost:877');  // Use actual IP if needed
  let pc;  // RTCPeerConnection

  ws.onerror = err => console.error('WebSocket error:', err);
  ws.onopen = () => {
    console.log('Signaling WebSocket connected');
    // Create RTCPeerConnection once WS is ready
    const config = { iceServers: [{ urls: "stun:stun.l.google.com:19302" }] };
    pc = new RTCPeerConnection(config);
    
    // When a remote track arrives, attach it to the video element
    pc.ontrack = event => {
      console.log('Remote track received');
      const videoEl = document.querySelector('.streamVideo');
      videoEl.srcObject = event.streams[0];
      // å»¶æ—¶è°ƒç”¨ play()ï¼Œç¡®ä¿æ²¡æœ‰å…¶ä»– load æ“ä½œå¹²æ‰°
      setTimeout(() => {
        videoEl.play().catch(err => console.error('Video play error:', err));
      }, 100);
    };
    
    // Forward local ICE candidates to TouchDesigner via WebSocket
    pc.onicecandidate = event => {
      if (event.candidate) {
        ws.send(JSON.stringify({
          signalingType: "Ice",
          content: {
            sdpCandidate: event.candidate.candidate,
            sdpMLineIndex: event.candidate.sdpMLineIndex,
            sdpMid: event.candidate.sdpMid
          }
        }));
      }
    };
  };

  // Handle signaling messages from TouchDesigner
  ws.onmessage = async (message) => {
    const msg = JSON.parse(message.data);
    console.log('Signaling message received:', msg);
    switch(msg.signalingType) {
      case "Offer":
        console.log("Received offer from TD");
        await pc.setRemoteDescription({ type: "offer", sdp: msg.content.sdp });
        const answer = await pc.createAnswer();
        await pc.setLocalDescription(answer);
        // Send answer back to TD
        ws.send(JSON.stringify({ signalingType: "Answer", content: { sdp: answer.sdp } }));
        break;
      case "Answer":
        console.log("Received answer from TD");
        await pc.setRemoteDescription({ type: "answer", sdp: msg.content.sdp });
        break;
      case "Ice":
        console.log("Received ICE candidate from TD");
        const cand = msg.content;
        await pc.addIceCandidate(new RTCIceCandidate({
          candidate: cand.sdpCandidate, 
          sdpMLineIndex: cand.sdpMLineIndex, 
          sdpMid: cand.sdpMid 
        }));
        break;
      case "Clients":
        console.log("Signaling server client list update:", msg.content);
        break;
    }
  };
</script>

</body>

</html>