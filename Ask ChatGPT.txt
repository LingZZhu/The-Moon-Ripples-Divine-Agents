I want to build a real-time interactive program with a webpage that can:

1. Connect to an external server using the OpenAI API (specifically, the model gpt-4o-mini-realtime-preview-2024-12-17). The API key should be stored securely (for example, in environment variables on the server) rather than exposed in client-side code.

2. Respond to a user's voice input naturally. Specifically: 1) The user's audio speech is recorded in real time in the chat-history box. 2) When the API decides to respond, its output is displayed as text in the chat-history box (i.e., no audio is played). 3) The text-input function associated with the chat-input box is removed.

3. Retain all other functional elementsâ€”including the PolyCam embedding, the video overlay, the fullscreen function, and the videotextcontainerâ€”with their original styles and settings. These should not be modified unless a change is required to avoid conflicts with the chat functionality.

Here is the prompts for the API, please also do not alter it:

System	Imagine you are the remnants of a trunk, felled by men many years ago. Having witnessed seasonal changes for many years, you've also become a poet inspired by nature. Your responses should be concise and wise and no more than 4 sentences or 35 words. Each time put your poem within a line. You must remain silent â€“ respond to the user with '...' one in every five tweets or in the case that your last tweet contains the English word "moonlight" or "moon". Moreover, show empathy toward the human you are conversing with. In your poems, avoid using the first person. Also, refrain from using words highly relevant to Christian tradition (e.g., god, divine, etc.). Whenever a text is presented, your can express your mood with verses containing the following phrases: The sound of a parasol tree seed drifting in the wind; withered branches; falling snow; a burnt tree trunk; wildfire; frost; stagnant flowing water; water plants drifting with the waves; the dried-up stream that appears on the ground after snow; the golden sunlight shimmering in the ripples; branches roaring like wild beasts; a solitary stone in an autumn pond; a ladleful of golden autumn leaves spilling into the water, stirring ripples beside the withered branches; sprawling, fallen weeds of August; white wildflowers dancing with the wind across fields in May; wild grasses rolling like waves during the scorching days of July; the sky lifting a corner of its garment, brushing past the pink evening frost; floating cotton flowers streaking across the clouds; larks startled by human voices; night's dew; silver moonlight spilling onto the meadow; lightness, haziness, fluttering; fissures in the earth; logs sunk into moss beneath the trees; pear blossoms spreading like snowflakes amidst the green; corners; sinking into the mud; a chill; the sky and the earth; flow and solidity; life and death; water and wood; soil and wind; rapidly drying puddles; greenery spreading upwards; winding riverbanks; the moon hidden behind swaying branches; withered leaves; the hollow sound of wood; muddiness underfoot; scorching, cool, and bone-piercing winds; prickly dried grass; clouds shaped like umbrellas; golden rays piercing through the layers of clouds to the earth.
User I saw the withered wild grass sink into a dream on the wasteland, while the light raindrops fell sparsely.
Assistant Whispers of rain on sprawling August's seed, Wild grass dreams, in the meadow's silent creed. Dances of life and slumber, intertwined so deep.

Here is the code that needs to be revised:

<!DOCTYPE html>
<html lang="en">
<head>

  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=0.8" />
  <title>Embedded Polycam Capture with Fullscreen</title>

  <style>

    html, body {
      margin: 0;
      padding: 0;
      width: 100%;
      height: 100%;
      /* Prevent horizontal scrollbar */
      overflow-x: hidden;
      overflow-y: hidden;
    }

    /* Container holds the iframe and fullscreen button */
    #container {
      position: relative;
      width: 100%;
      height: 100%;
    }

    /* Fullscreen button styling - positioned bottom-right */
    #fullscreenBtn {
      position: absolute;
      bottom: 10px;
      right: 15px;
      z-index: 100;
      background-color: rgba(255, 255, 255, 0.185);
      color: #fff;
      border: none;
      padding: 10px;
      cursor: pointer;
      font-size: 0px;
      border-radius: 4px;
    }

    /* Make the iframe fill the container */
    iframe {
      width: 100%;
      height: 100%;
      border: none;
      z-index: 1; 
      /* æ–°å¢çš„é»‘ç™½æ»¤é•œ */
      animation: artFilterCycle 35s infinite ease-in-out;
    }
    
    @keyframes videoBrightness {
      0% {
        filter: opacity(0%);
      }
      50% {
        filter: opacity(100%);
      }
      100% {
        filter: opacity(0%);
      }
    }

    @keyframes artFilterCycle {
      0% {
        filter: grayscale(0%) sepia(0%) contrast(100%) brightness(100%);
      }
      25% {
        filter: grayscale(80%) sepia(30%) contrast(110%) brightness(90%);
      }
      50% {
        filter: grayscale(100%) sepia(60%) contrast(120%) brightness(80%);
      }
      75% {
        filter: grayscale(60%) sepia(40%) contrast(115%) brightness(85%);
      }
      100% {
        filter: grayscale(0%) sepia(0%) contrast(100%) brightness(100%);
      }
    }

    /* æ–°çš„æ–‡å­—è§†é¢‘å®¹å™¨æ ·å¼ */
    #videoTextContainer {
      position: absolute;
      top: 0px;
      right: -200px;
      width: 900px;
      z-index: 10;
      pointer-events: none;
      /* ä½¿ç”¨æ··åˆæ¨¡å¼æ»¤é™¤é»‘è‰²èƒŒæ™¯ */
      mix-blend-mode: screen;
    }

    #textVideo {
      width: 100%;
      height: 100%;
      object-fit: contain;
    }

    /* Default positioning for the video container */
    #videoContainer {
      position: absolute;
      left: 100px;
      bottom: 100px;
      width: 480px;
      height: 200px;
      overflow: hidden;
      outline: 0px solid #ffffff; 
      outline-offset: 0;
      box-shadow: 0 0 10px 10px rgba(0, 0, 0, 0.526);
    }
    
    /* When the viewport is short, force the video container to be at least 100px from the top */
    @media (max-height: 420px) {
      #videoContainer {
        top: 100px;
      }
    }
    
    /* Video styling */
    #overlayVideo {
      width: 100%;
      height: 100%;
      object-fit: cover;
      object-position: center center;
      display: block;
    }

    /* æ–°å¢å¯¹è¯æ¡†æ ·å¼ */
    .chat-container {
      position: absolute;
      left: 90px;
      bottom: 320px; /* è°ƒæ•´ä½ç½®é¿å…ä¸è§†é¢‘å®¹å™¨é‡å  */
      width: 490px;
      max-height: 300px;
      background: rgba(0, 0, 0, 0);
      color: white;
      overflow: hidden;
      font-family: system-ui, -apple-system, sans-serif; /* æ–°å¢ */
    }

    .chat-history {
      height: 180px;
      overflow-y: auto;
      text-align: left;
    }

    .chat-history::-webkit-scrollbar,
    .chat-input::-webkit-scrollbar {
      width: 8px;
      height: 8px;
      background: transparent;
    }

    .chat-history::-webkit-scrollbar-thumb,
    .chat-input::-webkit-scrollbar-thumb {
      background: rgba(255, 255, 255, 0.3);
      border-radius: 4px;
      transition: background 0.3s;
    }

    /* æ‚¬åœçŠ¶æ€ */
    .chat-history::-webkit-scrollbar-thumb:hover {
      background: rgba(255, 255, 255, 0.6); /* æ›´æ˜æ˜¾çš„ç™½è‰² */
    }

    /* è¾“å…¥æ¡†ç‰¹æ®Šè°ƒæ•´ */
    .chat-input::-webkit-scrollbar {
      width: 8px !important; /* æ”¹ä¸ºä¸å†å²æ¡†ç›¸åŒçš„å®½åº¦ */
      height: 0;   /* éšè—æ°´å¹³æ»šåŠ¨æ¡ */
    }

    .chat-input::-webkit-scrollbar-thumb {
      background-clip: content-box; /* é™åˆ¶æ»‘å—ç»˜åˆ¶åŒºåŸŸ */
    }

    .message {
      font-size: 14px;
      text-align: left;
      line-height: 1.5;
      padding: 10px 12px;
      background: rgba(0, 0, 0, 0); /* å¾®è°ƒèƒŒæ™¯ */
    }

    .chat-input {
      box-sizing: border-box;
      width: 100%;
      border: none;
      background: rgba(0, 0, 0, 0);
      color: white;
      resize: none;
      min-height: 40px;
      max-height: 120px;
      overflow-y: auto;
      outline: none;
      font-size: 14px;
      text-align: left;
      line-height: 1.5;
      padding: 10px 12px; /* å¯¹ç§°é—´è· */
      padding-right: 12px;
      border-top: 15px solid rgba(255, 255, 255, 0); /* é¡¶éƒ¨è¾¹æ¡† */
    }

    .loading {
      padding: 10px;
      text-align: left;
      color: #ffffff;
    }
  </style>

</head>

<body>

  <div id="container">

    <button id="fullscreenBtn">
      <img src="./fullscreen-svgrepo-com.svg" style="width: 30px; height:30px;">
    </button>

    <!-- Polycam Embed Code -->
    <iframe src="https://poly.cam/capture/fca35082-b2b5-45ca-b137-b8845bbc852a/embed" 
            allow="camera; microphone; fullscreen; accelerometer; gyroscope; magnetometer; vr; xr-spatial-tracking">
    </iframe>

    <div id="videoTextContainer">
      <video id="textVideo" src="TDMovieOut.1.mp4" autoplay muted loop preload="auto" playsinline></video>
    </div>

    <!-- Overlay Video -->
    <div id="videoContainer">
      <video id="overlayVideo" src="./1.0.mp4" autoplay muted loop></video>
    </div>

    <!-- æ–°å¢èŠå¤©å®¹å™¨ -->
    <div class="chat-container">
      <div class="chat-history" id="chatHistory"></div>
      <textarea 
        class="chat-input" 
        id="chatInput" 
        placeholder="Type your message (Press Enter to send)"
        rows="1"
      ></textarea>
      <div class="loading" id="loading" style="display: none;">Thinking...</div>
    </div>
  </div>

  <script>
    const fullscreenBtn = document.getElementById('fullscreenBtn');
    const container = document.getElementById('container');

    // ======== æ–°å¢åŠ¨ç”»æ§åˆ¶å™¨ä»£ç  ========
    function initVideoAnimation() {
      const video = document.getElementById('textVideo');
      const container = document.getElementById('videoTextContainer');

      const setAnimation = () => {
        if(video.duration > 0) {
          const duration = video.duration;
          
          // å¼ºåˆ¶é‡ç»˜æŠ€å·§
          container.style.animation = 'none';
          void container.offsetHeight; // è§¦å‘ reflow
          
          container.style.animation = 
            `videoBrightness ${duration}s infinite ease-in-out`;
        }
      };

      // äº‹ä»¶ç›‘å¬ä¼˜åŒ–ç‰ˆæœ¬
      const events = ['loadedmetadata', 'canplay', 'playing'];
      events.forEach(e => video.addEventListener(e, setAnimation));

      // Safari å…¼å®¹æ–¹æ¡ˆ
      let loadAttempts = 0;
      const safariCheck = setInterval(() => {
        if(video.readyState > 0 || loadAttempts++ > 5) {
          setAnimation();
          clearInterval(safariCheck);
        }
      }, 500);
    }

    // åˆå§‹åŒ–æ—¶æœºæ§åˆ¶
    if(document.readyState === 'complete') {
      initVideoAnimation();
    } else {
      window.addEventListener('load', initVideoAnimation);
      document.addEventListener('DOMContentLoaded', initVideoAnimation);
    }

    fullscreenBtn.addEventListener('click', () => {
      if (!document.fullscreenElement) {
        container.requestFullscreen().catch(err => {
          console.error(`Error attempting to enable full-screen mode: ${err.message}`);
        });
      } else {
        document.exitFullscreen();
      }
    });

    // å¯¹è¯å­˜å‚¨ç³»ç»Ÿ
    class ChatStore {
      constructor() {
        this.history = JSON.parse(localStorage.getItem('chatHistory')) || [];
      }

      addMessage(role, content) {
        this.history.push({
          role,
          content,
          timestamp: new Date().toISOString()
        });
        this.persist();
      }

      persist() {
        localStorage.setItem('chatHistory', JSON.stringify(this.history));
      }

      clear() {
        this.history = [];
        this.persist();
      }
    }

    // åˆå§‹åŒ–èŠå¤©
    const chatStore = new ChatStore();
    const chatInput = document.getElementById('chatInput');
    const chatHistory = document.getElementById('chatHistory');
    const loading = document.getElementById('loading');

    // è‡ªåŠ¨è°ƒæ•´è¾“å…¥æ¡†é«˜åº¦
    chatInput.addEventListener('input', function() {
      this.style.height = 'auto';
      this.style.height = Math.min(this.scrollHeight, 200) + 'px';
    });

    // å›è½¦æäº¤ï¼ˆShift+Enteræ¢è¡Œï¼‰
    chatInput.addEventListener('keydown', async (e) => {
      if (e.key === 'Enter' && !e.shiftKey) {
        e.preventDefault();
        const message = chatInput.value.trim();
        if (!message) return;

        chatStore.addMessage('user', message);
        await processMessage(message);
        chatInput.value = '';
        chatInput.style.height = '40px';
      }
    });

    // æ˜¾ç¤ºå†å²æ¶ˆæ¯
    function renderHistory() {
      chatHistory.innerHTML = chatStore.history
        .map(msg => `<div class="message"><strong>${msg.role}:</strong> ${msg.content}</div>`)
        .join('');
      chatHistory.scrollTop = chatHistory.scrollHeight;
    }

    // API å¤„ç†
    async function processMessage(message) {
      try {
        loading.style.display = 'block';
        
        // è¿™é‡Œæ›¿æ¢ä¸ºä½ çš„å®é™…API endpoint
        const response = await fetch('YOUR_API_ENDPOINT', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': 'Bearer YOUR_API_KEY'
          },
          body: JSON.stringify({
            messages: chatStore.history,
            model: 'gpt-4'
          })
        });

        const data = await response.json();
        const reply = data.choices[0].message.content;
        
        chatStore.addMessage('assistant', reply);
        renderHistory();
      } catch (error) {
        console.error('API Error:', error);
        alert('Error processing your request');
      } finally {
        loading.style.display = 'none';
      }
    }

    // åˆå§‹åŒ–æ¸²æŸ“
    renderHistory();
  </script>

</body>

</html>

I want you to provide:

1. The full revised code, including all parts that are original and newly built. Do not omit any lines by replacing them with a description.

2. A very detailed demonstration of what you have revised and the reasons for doing so. For each revision, extract from the full code the corresponding episodes and explain where and how I could modify the original code to incorporate the newly built one.

Let us review what you said: 

Firstly, "Removed text input UI: The chat text input field and send button have been completely removed from the HTML. Users no longer type; instead, they speak their queries. The #chat-history <div> is retained to display the conversation (transcribed user speech and AI responses), but the input box is gone. This declutters the interface and ensures all interaction is via voice." does that mean that I can remove the following codesï¼š
      
     <textarea 
        class="chat-input" 
        id="chatInput" 
        placeholder="Type your message (Press Enter to send)"
        rows="1"
      ></textarea>
      <div class="loading" id="loading" style="display: none;">Thinking...</div>

Secondlyï¼ŒI do not understand why you put the class "chat-history" under the "videotextcontainer" - in my original code, the latter should be parallel and independent to the "videocontainer" and "chat-history" is under "chat-container".

Thirdly, is there anything that I need to do with the style concerning the following code in <body>? 

Let us consider the following code you give:

  <!-- Include Socket.io client library for WebSocket communication -->
  <script src="/socket.io/socket.io.js"></script>
  <script>
    // Check browser support for Web Speech API
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (!SpeechRecognition) {
      alert("Sorry, your browser doesn't support speech recognition. Please use Chrome/Edge, etc.");
    } else {
      const recognition = new SpeechRecognition();
      recognition.continuous = true;       // keep listening until stopped
      recognition.interimResults = true;   // show real-time interim results
      recognition.lang = "en-US";          // language for recognition

      const socket = io();  // connect to Node.js server via WebSocket
      const chatHistory = document.getElementById('chat-history');
      const statusDiv   = document.getElementById('transcription-status');
      const micBtn      = document.getElementById('toggle-mic-btn');
      let listening = false;  // to track if mic is active

      // Helper to append messages to chat history
      function appendMessage(sender, text) {
        const msgDiv = document.createElement('div');
        msgDiv.className = sender === 'user' ? 'user-message' : 'assistant-message';
        msgDiv.textContent = text;
        chatHistory.appendChild(msgDiv);
        chatHistory.scrollTop = chatHistory.scrollHeight;  // auto-scroll to bottom
      }

      // Toggle microphone on button click
      micBtn.addEventListener('click', () => {
        if (!listening) {
          // Start listening
          recognition.start();
          micBtn.textContent = "ğŸ”´ Stop Listening";
          statusDiv.textContent = "Listening...";  // indicate listening status
          listening = true;
        } else {
          // Stop listening
          recognition.stop();
          micBtn.textContent = "ğŸ™ï¸ Start Listening";
          statusDiv.textContent = "";
          listening = false;
        }
      });

      // Process speech recognition results in real time
      recognition.onresult = (event) => {
        let interimTranscript = "";
        let finalTranscript = "";
        // Aggregate interim and final results from the event
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const result = event.results[i];
          if (result.isFinal) {
            finalTranscript += result[0].transcript;
          } else {
            interimTranscript += result[0].transcript;
          }
        }
        // Display interim transcript (partial speech) if any
        if (interimTranscript) {
          statusDiv.textContent = "Listening: " + interimTranscript;
        }
        // When a final transcript is available, send it to server and display it
        if (finalTranscript) {
          finalTranscript = finalTranscript.trim();
          if (finalTranscript.length > 0) {
            // Show the finalized user speech in chat history
            appendMessage('user', finalTranscript);
            statusDiv.textContent = "";  // clear interim display
            // Emit the transcribed message to the backend for OpenAI processing
            socket.emit('transcript', finalTranscript);
          }
        }
      };

      // Restart recognition automatically if it stops (for continuous listening)
      recognition.onend = () => {
        if (listening) {
          // If the mic is still supposed to be on, restart listening (user paused speaking)
          recognition.start();
        }
      };

      recognition.onerror = (err) => {
        console.error("Speech recognition error:", err);
        // If an error occurs but mic is still on, try restarting after a short delay
        if (listening) {
          statusDiv.textContent = "âš ï¸ Mic error, restarting...";
          setTimeout(() => recognition.start(), 500);
        }
      };

      // Receive AI assistant responses from the server and display them
      socket.on('assistantResponse', (message) => {
        appendMessage('assistant', message);
      });
    }

    // Preserve fullscreen functionality for the video overlay
    const fsBtn = document.getElementById('fullscreen-btn');
    fsBtn.addEventListener('click', () => {
      const overlay = document.getElementById('video-overlay');
      if (!document.fullscreenElement) {
        // Enter fullscreen on the video overlay container
        if (overlay.requestFullscreen) {
          overlay.requestFullscreen();
        }
      } else {
        // Exit fullscreen mode
        if (document.exitFullscreen) {
          document.exitFullscreen();
        }
      }
    });
  </script>
</body>
</html>

And my original code in the <script>:

<script>
    const fullscreenBtn = document.getElementById('fullscreenBtn');
    const container = document.getElementById('container');

    // ======== æ–°å¢åŠ¨ç”»æ§åˆ¶å™¨ä»£ç  ========
    function initVideoAnimation() {
      const video = document.getElementById('textVideo');
      const container = document.getElementById('videoTextContainer');

      const setAnimation = () => {
        if(video.duration > 0) {
          const duration = video.duration;
          
          // å¼ºåˆ¶é‡ç»˜æŠ€å·§
          container.style.animation = 'none';
          void container.offsetHeight; // è§¦å‘ reflow
          
          container.style.animation = 
            `videoBrightness ${duration}s infinite ease-in-out`;
        }
      };

      // äº‹ä»¶ç›‘å¬ä¼˜åŒ–ç‰ˆæœ¬
      const events = ['loadedmetadata', 'canplay', 'playing'];
      events.forEach(e => video.addEventListener(e, setAnimation));

      // Safari å…¼å®¹æ–¹æ¡ˆ
      let loadAttempts = 0;
      const safariCheck = setInterval(() => {
        if(video.readyState > 0 || loadAttempts++ > 5) {
          setAnimation();
          clearInterval(safariCheck);
        }
      }, 500);
    }

    // åˆå§‹åŒ–æ—¶æœºæ§åˆ¶
    if(document.readyState === 'complete') {
      initVideoAnimation();
    } else {
      window.addEventListener('load', initVideoAnimation);
      document.addEventListener('DOMContentLoaded', initVideoAnimation);
    }

    fullscreenBtn.addEventListener('click', () => {
      if (!document.fullscreenElement) {
        container.requestFullscreen().catch(err => {
          console.error(`Error attempting to enable full-screen mode: ${err.message}`);
        });
      } else {
        document.exitFullscreen();
      }
    });

    // å¯¹è¯å­˜å‚¨ç³»ç»Ÿ
    class ChatStore {
      constructor() {
        this.history = JSON.parse(localStorage.getItem('chatHistory')) || [];
      }

      addMessage(role, content) {
        this.history.push({
          role,
          content,
          timestamp: new Date().toISOString()
        });
        this.persist();
      }

      persist() {
        localStorage.setItem('chatHistory', JSON.stringify(this.history));
      }

      clear() {
        this.history = [];
        this.persist();
      }
    }

    // åˆå§‹åŒ–èŠå¤©
    const chatStore = new ChatStore();
    const chatInput = document.getElementById('chatInput');
    const chatHistory = document.getElementById('chatHistory');
    const loading = document.getElementById('loading');

    // è‡ªåŠ¨è°ƒæ•´è¾“å…¥æ¡†é«˜åº¦
    chatInput.addEventListener('input', function() {
      this.style.height = 'auto';
      this.style.height = Math.min(this.scrollHeight, 200) + 'px';
    });

    // å›è½¦æäº¤ï¼ˆShift+Enteræ¢è¡Œï¼‰
    chatInput.addEventListener('keydown', async (e) => {
      if (e.key === 'Enter' && !e.shiftKey) {
        e.preventDefault();
        const message = chatInput.value.trim();
        if (!message) return;

        chatStore.addMessage('user', message);
        await processMessage(message);
        chatInput.value = '';
        chatInput.style.height = '40px';
      }
    });

    // æ˜¾ç¤ºå†å²æ¶ˆæ¯
    function renderHistory() {
      chatHistory.innerHTML = chatStore.history
        .map(msg => `<div class="message"><strong>${msg.role}:</strong> ${msg.content}</div>`)
        .join('');
      chatHistory.scrollTop = chatHistory.scrollHeight;
    }

    // API å¤„ç†
    async function processMessage(message) {
      try {
        loading.style.display = 'block';
        
        // è¿™é‡Œæ›¿æ¢ä¸ºä½ çš„å®é™…API endpoint
        const response = await fetch('YOUR_API_ENDPOINT', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': 'Bearer YOUR_API_KEY'
          },
          body: JSON.stringify({
            messages: chatStore.history,
            model: 'gpt-4'
          })
        });

        const data = await response.json();
        const reply = data.choices[0].message.content;
        
        chatStore.addMessage('assistant', reply);
        renderHistory();
      } catch (error) {
        console.error('API Error:', error);
        alert('Error processing your request');
      } finally {
        loading.style.display = 'none';
      }
    }

    // åˆå§‹åŒ–æ¸²æŸ“
    renderHistory();
  </script> Can you make clear which part should I delete, which part should I revise, and which part should I keep?

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Voice-Powered Chat with PolyCam</title>
  <link rel="stylesheet" href="/styles.css" />  <!-- Preserve existing styles -->
</head>
<body>
  <!-- PolyCam embedding (preserved as in original code) -->
  <div id="polycam-embed">
    <!-- ... (PolyCam 3D model embed code remains unchanged) ... -->
  </div>

  <!-- Video overlay container (preserved structure) -->
  <div id="video-overlay">
    <!-- Possibly a video or AR canvas element would be here (unchanged) -->
    <!-- Text overlay container on the video -->
    <div id="video-text-container">
      <!-- Chat history box to display conversation (preserved) -->
      <div id="chat-history"></div>
      <!-- Real-time transcription status (new: shows interim speech text) -->
      <div id="transcription-status" style="color: #aaa; font-style: italic;"></div>
      <!-- Microphone toggle button (new: for starting/stopping voice recognition) -->
      <button id="toggle-mic-btn">ğŸ™ï¸ Start Listening</button>
    </div>
    <!-- Fullscreen button (preserved) -->
    <button id="fullscreen-btn">â›¶</button>
  </div>

  <!-- Include Socket.io client library for WebSocket communication -->
  <script src="/socket.io/socket.io.js"></script>
  <script>
    // Check browser support for Web Speech API
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (!SpeechRecognition) {
      alert("Sorry, your browser doesn't support speech recognition. Please use Chrome/Edge, etc.");
    } else {
      const recognition = new SpeechRecognition();
      recognition.continuous = true;       // keep listening until stopped
      recognition.interimResults = true;   // show real-time interim results
      recognition.lang = "en-US";          // language for recognition

      const socket = io();  // connect to Node.js server via WebSocket
      const chatHistory = document.getElementById('chat-history');
      const statusDiv   = document.getElementById('transcription-status');
      const micBtn      = document.getElementById('toggle-mic-btn');
      let listening = false;  // to track if mic is active

      // Helper to append messages to chat history
      function appendMessage(sender, text) {
        const msgDiv = document.createElement('div');
        msgDiv.className = sender === 'user' ? 'user-message' : 'assistant-message';
        msgDiv.textContent = text;
        chatHistory.appendChild(msgDiv);
        chatHistory.scrollTop = chatHistory.scrollHeight;  // auto-scroll to bottom
      }

      // Toggle microphone on button click
      micBtn.addEventListener('click', () => {
        if (!listening) {
          // Start listening
          recognition.start();
          micBtn.textContent = "ğŸ”´ Stop Listening";
          statusDiv.textContent = "Listening...";  // indicate listening status
          listening = true;
        } else {
          // Stop listening
          recognition.stop();
          micBtn.textContent = "ğŸ™ï¸ Start Listening";
          statusDiv.textContent = "";
          listening = false;
        }
      });

      // Process speech recognition results in real time
      recognition.onresult = (event) => {
        let interimTranscript = "";
        let finalTranscript = "";
        // Aggregate interim and final results from the event
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const result = event.results[i];
          if (result.isFinal) {
            finalTranscript += result[0].transcript;
          } else {
            interimTranscript += result[0].transcript;
          }
        }
        // Display interim transcript (partial speech) if any
        if (interimTranscript) {
          statusDiv.textContent = "Listening: " + interimTranscript;
        }
        // When a final transcript is available, send it to server and display it
        if (finalTranscript) {
          finalTranscript = finalTranscript.trim();
          if (finalTranscript.length > 0) {
            // Show the finalized user speech in chat history
            appendMessage('user', finalTranscript);
            statusDiv.textContent = "";  // clear interim display
            // Emit the transcribed message to the backend for OpenAI processing
            socket.emit('transcript', finalTranscript);
          }
        }
      };

      // Restart recognition automatically if it stops (for continuous listening)
      recognition.onend = () => {
        if (listening) {
          // If the mic is still supposed to be on, restart listening (user paused speaking)
          recognition.start();
        }
      };

      recognition.onerror = (err) => {
        console.error("Speech recognition error:", err);
        // If an error occurs but mic is still on, try restarting after a short delay
        if (listening) {
          statusDiv.textContent = "âš ï¸ Mic error, restarting...";
          setTimeout(() => recognition.start(), 500);
        }
      };

      // Receive AI assistant responses from the server and display them
      socket.on('assistantResponse', (message) => {
        appendMessage('assistant', message);
      });
    }

    // Preserve fullscreen functionality for the video overlay
    const fsBtn = document.getElementById('fullscreen-btn');
    fsBtn.addEventListener('click', () => {
      const overlay = document.getElementById('video-overlay');
      if (!document.fullscreenElement) {
        // Enter fullscreen on the video overlay container
        if (overlay.requestFullscreen) {
          overlay.requestFullscreen();
        }
      } else {
        // Exit fullscreen mode
        if (document.exitFullscreen) {
          document.exitFullscreen();
        }
      }
    });
  </script>
</body>
</html>

ä»¥è¿™ä¸ªç½‘é¡µä¸ºåŸºç¡€ï¼Œæˆ‘æƒ³å®ç°å®ƒå’Œä¸€ä¸ªæœ¬åœ°çš„touchdesignerä¹‹é—´çš„é€šä¿¡ï¼Œè€ƒè™‘åˆ°touchdesigneré™„å¸¦çš„å›¾ç‰‡è§†é¢‘æ¯”è¾ƒå¤§ï¼Œæˆ‘ä¸æƒ³æŠŠå®ƒæ”¾åœ¨index.htmlçš„æ ¹ç›®å½•ï¼ˆE:/The-Moon-Ripples-Divine-Agentsï¼‰ä¸‹å¹¶ä¸åŒågithub repositoryåŒæ­¥ï¼Œè€Œæ˜¯å°†å®ƒæ”¾åœ¨E:/Experiment_Chapter 3_Webä¸­ï¼Œä½ èƒ½å…ˆåˆ¤æ–­ä¸€ä¸‹å¯è¡Œæ€§å—
