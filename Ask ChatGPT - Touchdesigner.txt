I have a TouchDesigner program which contains an TOP operator of real-time generated animation. Now I want to livestream this animation on a local webpage. Here is some details for your reference:

1ï¼‰The local webpage is stored in E:/The-Moon-Ripples-Divine-agents, which includs two pieces of codes:

iï¼‰index.html
iiï¼‰app.js, which is running on Port: 3000

2) The resolution of real-time generated animation is 960*540, and I would like to place it into a circular (diameter = 720 px) video container situated at the centre of the webpage.

3) Here is the original code of index.html and app.js, I want to you to remain all the original functional elements:

index.html:

<!DOCTYPE html>
<html lang="en">
<head>

  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Voice-Powered Chat with PolyCam</title>

  <style>

    html, body {
      margin: 0;
      padding: 0;
      width: 100%;
      height: 100%;
      /* Prevent horizontal scrollbar */
      overflow-x: hidden;
      overflow-y: hidden;
    }

    /* Container holds the iframe and fullscreen button */
    #container {
      position: relative;
      width: 100%;
      height: 100%;
    }

    /* Fullscreen button styling - positioned bottom-right */
    #fullscreenBtn {
      position: absolute;
      bottom: 10px;
      right: 18px;
      z-index: 100;
      background-color: rgba(255, 255, 255, 0.185);
      color: #fff;
      border: none;
      padding: 5px;
      cursor: pointer;
      font-size: 0px;
      border-radius: 4px;
    }

    /* Make the iframe fill the container */
    iframe {
      width: 100%;
      height: 100%;
      border: none;
      z-index: 1; 
      /* æ–°å¢çš„é»‘ç™½æ»¤é•œ */
      animation: artFilterCycle 35s infinite ease-in-out;
    }
    
    @keyframes videoBrightness {
      0% {
        filter: opacity(0%);
      }
      50% {
        filter: opacity(100%);
      }
      100% {
        filter: opacity(0%);
      }
    }

    @keyframes artFilterCycle {
      0% {
        filter: grayscale(0%) sepia(0%) contrast(100%) brightness(100%);
      }
      25% {
        filter: grayscale(80%) sepia(30%) contrast(110%) brightness(90%);
      }
      50% {
        filter: grayscale(100%) sepia(60%) contrast(120%) brightness(80%);
      }
      75% {
        filter: grayscale(60%) sepia(40%) contrast(115%) brightness(85%);
      }
      100% {
        filter: grayscale(0%) sepia(0%) contrast(100%) brightness(100%);
      }
    }

    /* æ–°çš„æ–‡å­—è§†é¢‘å®¹å™¨æ ·å¼ */
    #videoTextContainer {
      position: absolute;
      top: 0px;
      right: -200px;
      width: 900px;
      z-index: 10;
      pointer-events: none;
      /* ä½¿ç”¨æ··åˆæ¨¡å¼æ»¤é™¤é»‘è‰²èƒŒæ™¯ */
      mix-blend-mode: screen;
    }

    #textVideo {
      width: 100%;
      height: 100%;
      object-fit: contain;
    }

    /* Default positioning for the video container */
    #videoContainer {
      position: absolute;
      left: 100px;
      bottom: 100px;
      width: 480px;
      height: 200px;
      overflow: hidden;
      outline: 0px solid #ffffff; 
      outline-offset: 0;
      box-shadow: 0 0 10px 10px rgba(0, 0, 0, 0.526);
    }
    
    /* When the viewport is short, force the video container to be at least 100px from the top */
    @media (max-height: 420px) {
      #videoContainer {
        top: 100px;
      }
    }
    
    /* Video styling */
    #overlayVideo {
      width: 100%;
      height: 100%;
      object-fit: cover;
      object-position: center center;
      display: block;
    }

    /* ä¿®æ”¹åçš„èŠå¤©å®¹å™¨æ ·å¼ */
    .chat-container {
        position: absolute;
        left: 100px;
        bottom: 320px;
        width: 480px;
        height: 300px; /* å›ºå®šé«˜åº¦æ›¿ä»£max-height */
        background: rgba(0, 0, 0, 0);
        color: white;
        overflow: hidden;
        font-family: system-ui, -apple-system, sans-serif;
        display: flex; /* æ–°å¢flexå¸ƒå±€ */
        flex-direction: column; /* å‚ç›´æ’åˆ— */
    }

    .chat-history {
      height:288px;
      overflow-y: auto;
      text-align: left;
      padding-bottom: 45px;
      font-size: 10.5px;
    }

    .chat-history::-webkit-scrollbar {
      width: 8px;
      height: 8px;
      background: transparent;
    }

    .chat-history::-webkit-scrollbar-thumb {
      background: rgba(255, 255, 255, 0.3);
      border-radius: 4px;
      transition: box-shadow 0.3s ease; 
    }

    /* æ‚¬åœçŠ¶æ€ */
    .chat-history::-webkit-scrollbar-thumb:hover {
      background: rgba(255, 255, 255, 0.6); /* æ›´æ˜æ˜¾çš„ç™½è‰² */
    }

    #transcription-status {
      color: #aaa;
      font-style: italic;
      margin-bottom: 10px;
      font-size: 10.5px;
    }

    /* ä¿®æ”¹åçš„æŒ‰é’®å®šä½ */
    #toggle-mic-btn {
      position: relative; /* æ”¹ä¸ºç›¸å¯¹å®šä½ */
      left: 0;
      bottom: 0;
      margin-top: auto; /* è‡ªåŠ¨æ¨åˆ°å®¹å™¨åº•éƒ¨ */
      width: min-content;
      min-width: 120px; /* æœ€å°å®½åº¦ä¿è¯æ–‡å­—ä¸æ¢è¡Œ */
      white-space: nowrap; /* é˜²æ­¢æ–‡å­—æ¢è¡Œ */
      z-index: 10;
      background-color: rgba(255, 255, 255, 0.6);
      color: #fff;
      border: none;
      padding: 8px 16px;
      border-radius: 0px;
      font-size: 12px;
      cursor: pointer;
    }

  </style>

</head>

<body>

  <div id="container">

    <button id="fullscreenBtn">
      <img src="./fullscreen-svgrepo-com.svg" style="width: 30px; height:30px;">
    </button>

    <!-- Polycam Embed Code -->
    <iframe src="https://poly.cam/capture/fca35082-b2b5-45ca-b137-b8845bbc852a/embed" 
            allow="camera; microphone; fullscreen; accelerometer; gyroscope; magnetometer; vr; xr-spatial-tracking">
    </iframe>

    <div id="videoTextContainer">
      <video id="textVideo" src="TDMovieOut.1.mp4" autoplay muted loop preload="auto" playsinline></video>
    </div>

    <!-- Overlay Video -->
    <div id="videoContainer">
      <video id="overlayVideo" src="./1.0.mp4" autoplay muted loop></video>
    </div>

    <!-- æ–°å¢èŠå¤©å®¹å™¨ -->
    <div class="chat-container">
      <div class="chat-history" id="chat-history"></div>
      <!-- New voice interface elements -->
      <div id="transcription-status">
        <!-- Real-time transcription will appear here -->
      </div>
      <button id="toggle-mic-btn">
        ğŸ™ï¸ Start Listening
      </button>
    </div>
  </div>

  <script>
    // åœ¨é¡µé¢åŠ è½½åå¼ºåˆ¶è®¾ç½®å¾ªç¯
    window.addEventListener('load', () => {
      const videos = document.querySelectorAll('video');
      videos.forEach(video => {
        video.loop = true; // æ˜¾å¼è®¾ç½®å¾ªç¯
        
        // å…¼å®¹æ€§å¤„ç†
        video.addEventListener('ended', () => {
          video.currentTime = 0;
          video.play();
        });
      });
    });
  </script>

  <script>
    const fullscreenBtn = document.getElementById('fullscreenBtn');
    const container = document.getElementById('container');

    // ======== æ–°å¢åŠ¨ç”»æ§åˆ¶å™¨ä»£ç  ========
    function initVideoAnimation() {
      const video = document.getElementById('textVideo');
      const container = document.getElementById('videoTextContainer');

      const setAnimation = () => {
        if(video.duration > 0) {
          const duration = video.duration;
          
          // å¼ºåˆ¶é‡ç»˜æŠ€å·§
          container.style.animation = 'none';
          void container.offsetHeight; // è§¦å‘ reflow
          
          container.style.animation = 
            `videoBrightness ${duration}s infinite ease-in-out`;
        }
      };

      // äº‹ä»¶ç›‘å¬ä¼˜åŒ–ç‰ˆæœ¬
      const events = ['loadedmetadata', 'canplay', 'playing'];
      events.forEach(e => video.addEventListener(e, setAnimation));

      // Safari å…¼å®¹æ–¹æ¡ˆ
      let loadAttempts = 0;
      const safariCheck = setInterval(() => {
        if(video.readyState > 0 || loadAttempts++ > 5) {
          setAnimation();
          clearInterval(safariCheck);
        }
      }, 500);
    }

    // åˆå§‹åŒ–æ—¶æœºæ§åˆ¶
    if(document.readyState === 'complete') {
      initVideoAnimation();
    } else {
      window.addEventListener('load', initVideoAnimation);
      document.addEventListener('DOMContentLoaded', initVideoAnimation);
    }

    fullscreenBtn.addEventListener('click', () => {
      if (!document.fullscreenElement) {
        container.requestFullscreen().catch(err => {
          console.error(`Error attempting to enable full-screen mode: ${err.message}`);
        });
      } else {
        document.exitFullscreen();
      }
    });

  </script>

  <!-- Include Socket.io client library for WebSocket communication -->
  <script src="/socket.io/socket.io.js"></script>

  <script>
    // ========== é€šä¿¡æ¨¡å— ==========
    let lastCheckTime = Date.now();
    let lastCharCount = 0;
    let accumulatedBuffer = "";
    const CHAR_THRESHOLD = 100;
    const CHECK_INTERVAL = 90000;
    
    // ========== å…¨å±€é€šä¿¡æ¨¡å— ==========
    let socket; // å…¨å±€å£°æ˜

    window.addEventListener('load', () => {
      // åˆå§‹åŒ– Socket è¿æ¥ï¼ˆç¡®ä¿æœåŠ¡å™¨åœ°å€æ­£ç¡®ï¼‰
      socket = io('http://localhost:3000'); 

      // æ·»åŠ è¿æ¥çŠ¶æ€ç›‘å¬
      socket.on('connect', () => {
        console.log('Socket connected:', socket.id);
        socket.on('assistantResponse', (message) => {
          appendMessage('assistant', message);
        });
      });
      

      socket.on('disconnect', () => {
        console.log('Socket disconnected');
      });
    });

    // åˆå§‹åŒ–å­—ç¬¦ç»Ÿè®¡
    function initCharCounter() {
      const chatHistory = document.getElementById('chat-history');
      if (!chatHistory) return;
      lastCharCount = chatHistory.textContent.length;
    }

    // å®šæ—¶æ£€æµ‹å‡½æ•°
    function checkChatActivity() {
      const chatHistory = document.getElementById('chat-history');
      if (!chatHistory) return;
      const currentContent = chatHistory.textContent;
      const newChars = currentContent.slice(lastCharCount);
      accumulatedBuffer += newChars;
      lastCharCount = currentContent.length; // å…³é”®ä¿®å¤ï¼šå¿…é¡»æ›´æ–°
      if (accumulatedBuffer.length >= CHAR_THRESHOLD) {
        console.log("ğŸš€ è§¦å‘ä¼ è¾“:", accumulatedBuffer);
        socket.emit('td-generate', {
          timestamp: Date.now(),
          text: accumulatedBuffer
        });
        accumulatedBuffer = "";
      } else {
        console.log("â³ ç´¯ç§¯è¿›åº¦:", 
          `${accumulatedBuffer.length}/${CHAR_THRESHOLD}`,
          "æœ€æ–°å†…å®¹:", newChars
        );
      }
    }

    // å®šæ—¶å™¨ç®¡ç†
    let checkTimer;
    function startCheckTimer() {
      checkTimer = setInterval(checkChatActivity, CHECK_INTERVAL);
      checkChatActivity(); // ç«‹å³æ‰§è¡Œä¸€æ¬¡
    }

    // é¡µé¢ç”Ÿå‘½å‘¨æœŸç®¡ç†
    window.addEventListener('load', () => {
      initCharCounter();
      startCheckTimer();
    });

    document.addEventListener('visibilitychange', () => {
      if (document.hidden) {
        clearInterval(checkTimer);
      } else {
        startCheckTimer();
      }
    });

  </script>

  <script>

    // Check browser support for Web Speech API
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    const debounce = (func, delay) => {
      let timeoutId;
      return (...args) => {
        clearTimeout(timeoutId);
        timeoutId = setTimeout(() => func(...args), delay);
      };
    };

    if (!SpeechRecognition) {
      alert("Sorry, your browser doesn't support speech recognition. Please use Chrome/Edge, etc.");
    } else {
      const recognition = new SpeechRecognition();
      recognition.continuous = true;       // keep listening until stopped
      recognition.interimResults = true;   // show real-time interim results
      recognition.lang = "en-US";          // language for recognition

      
      let interimBuffer = ""; // æ·»åŠ è¿™ä¸€è¡Œ
      const chatHistory = document.getElementById('chat-history');
      const statusDiv   = document.getElementById('transcription-status');
      const micBtn      = document.getElementById('toggle-mic-btn');
      let listening = false;  // to track if mic is active

      // ä¿®æ”¹åçš„appendMessageå‡½æ•°
      function appendMessage(sender, text) {
        const msgDiv = document.createElement('div');
        msgDiv.className = sender === 'user' ? 'user-message' : 'assistant-message';
        msgDiv.textContent = text;
        
        // åœ¨æ·»åŠ å…ƒç´ å‰è·å–å½“å‰æ»šåŠ¨çŠ¶æ€
        const container = chatHistory;
        const wasAtBottom = container.scrollHeight - container.clientHeight <= container.scrollTop + 1;

        chatHistory.appendChild(msgDiv);

        // ä½¿ç”¨åŒRAFç¡®ä¿DOMæ›´æ–°å®Œæˆ
        requestAnimationFrame(() => {
          requestAnimationFrame(() => {
            // å¼ºåˆ¶æ»šåŠ¨åˆ°åº•éƒ¨
            container.scrollTop = container.scrollHeight;
                
            // å¦‚æœå½“å‰ä¸åœ¨åº•éƒ¨ï¼Œæ·»åŠ è§†è§‰æç¤º
            if (!wasAtBottom) {
              container.style.boxShadow = 'inset 0 -5px 10px rgba(255,255,255,0.3)';
              setTimeout(() => {
                container.style.boxShadow = 'none';
              }, 500);
            }
          });
        });
      }

      micBtn.addEventListener('click', async () => {
        try {
          if (!listening) {
            // æ˜¾å¼è¯·æ±‚éº¦å…‹é£æƒé™
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            stream.getTracks().forEach(track => track.stop());
      
            recognition.start();
            micBtn.textContent = "ğŸ”´ Stop Listening";
            statusDiv.textContent = "Listening...";
            listening = true;
          } else {
            recognition.stop();
            micBtn.textContent = "ğŸ™ï¸ Start Listening";
            statusDiv.textContent = "";
            listening = false;
          }
        } catch (err) {
          console.error('éº¦å…‹é£è®¿é—®é”™è¯¯:', err);
          statusDiv.textContent = "è¯·å…è®¸éº¦å…‹é£è®¿é—®";
          micBtn.textContent = "ğŸ™ï¸ Start Listening";
          listening = false;
        }
      });

      // å¢åŠ è¯­éŸ³è¯†åˆ«é”™è¯¯å¤„ç†
      recognition.onerror = (event) => {
        console.error('è¯†åˆ«é”™è¯¯:', event.error);
        if (event.error === 'not-allowed') {
          statusDiv.textContent = 'è¯·å…è®¸éº¦å…‹é£è®¿é—®';
        }
        listening = false;
        micBtn.textContent = "ğŸ™ï¸ Start Listening";
      };

      // ä¿®æ”¹åçš„å˜é‡
      let confirmedSegments = [];  // å­˜å‚¨æ‰€æœ‰å·²ç¡®è®¤çš„æ®µè½
      let currentDraft = "";       // å½“å‰ä¸´æ—¶è‰ç¨¿
      let silenceTimer = null;
      const SILENCE_TIMEOUT = 5000;

      // ç»Ÿä¸€æäº¤å‡½æ•°
      //function commitFinalTranscript() {
        //if (pendingFinal.trim().length > 0) {
          //handleFinalTranscript(pendingFinal);
          //pendingFinal = "";
        //}
        //clearTimeout(silenceTimer);
        //}

      function handleFinalTranscript(rawText) {

        // æ¸…æ´—æ­¥éª¤ï¼š
        let cleanText = rawText
          .replace(/(\b\w+\b)(\s+\1\b)+/gi, "$1") // å»é‡ï¼šå°†é‡å¤çš„å•è¯åˆå¹¶
          .replace(/\s{2,}/g, " ")                // åˆå¹¶å¤šä½™ç©ºæ ¼
          .trim();
        
        // 2. æ™ºèƒ½åˆ†æ®µï¼ˆé˜²æ­¢å¥å­æˆªæ–­ï¼‰
        const sentences = cleanText.match(/[^.!?]+[.!?]*/g) || [];
        cleanText = sentences.join(" ").trim();

        if (cleanText) {
          appendMessage('user', cleanText);
          socket.emit('transcript', cleanText);
          statusDiv.textContent = "å·²æäº¤ âœ“";
        }
      }

      let debugCount = 0;

      recognition.onresult = (event) => {
    
        console.log(`å¤„ç†ç»“æœ #${++debugCount}`, event);

        clearTimeout(silenceTimer); // å¿…é¡»é¦–å…ˆæ¸…é™¤æ—§è®¡æ—¶å™¨

        let hasNewFinal = false;
    
        // å¤„ç†æ‰€æœ‰ç»“æœ
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const result = event.results[i];
          const text = result[0].transcript.trim();
    
          if (result.isFinal) {
            // æœ€ç»ˆç»“æœï¼šå­˜å…¥å†å²æ®µè½
            confirmedSegments.push(text);
            hasNewFinal = true;
          } else {
            // ä¸´æ—¶ç»“æœï¼šæ›´æ–°å½“å‰è‰ç¨¿
            currentDraft = text;
          }
        }

        // æ„å»ºå®Œæ•´å†…å®¹
        let fullText = "";

        if (confirmedSegments.length > 0) {
          fullText = confirmedSegments.join(". ") //+ (currentDraft ? ` ${currentDraft}` : "");
        } else {
          fullText = currentDraft;
        }

        // æ›´æ–°æ˜¾ç¤º
        statusDiv.textContent = hasNewFinal 
          ? `å·²ç¡®è®¤æ®µè½: ${confirmedSegments.join(" | ")}`
          : currentDraft 
            ? `è¾“å…¥ä¸­: ${currentDraft}`
            : "";

        const debouncedHandle = debounce((text) => {
          handleFinalTranscript(text);
          confirmedSegments = [];
          currentDraft = "";
        }, 300); // 300msé˜²æŠ–çª—å£

        silenceTimer = setTimeout(() => debouncedHandle(fullText), 5000);
      };

      // Restart recognition automatically if it stops (for continuous listening)
      recognition.onend = () => {
        if (listening) {
          // If the mic is still supposed to be on, restart listening (user paused speaking)
          recognition.start();
        }
      };

      // éœ€è¦ä¿®æ”¹çš„é”™è¯¯å¤„ç†ä»£ç æ®µ
      recognition.onerror = (err) => {
        console.error("Speech recognition error:", err);
        clearTimeout(silenceTimer);
    
        // æ–°å¢é”™è¯¯ç±»å‹è¿‡æ»¤
        const ignorableErrors = [
          'no-speech',         // æ— è¯­éŸ³è¾“å…¥
          'audio-capture',     // éŸ³é¢‘æ•è·é—®é¢˜
          'network'            // ç½‘ç»œé”™è¯¯
        ];

        if (listening) {
          // å¯¹å¯å¿½ç•¥é”™è¯¯ä¸æ˜¾ç¤ºæç¤º
          if (!ignorableErrors.includes(err.error)) {
            statusDiv.textContent = "âš ï¸ Mic error, restarting...";
          }
        
          // ä¼˜åŒ–é‡å¯ç­–ç•¥
          const retry = () => {
            recognition.stop();  // å…ˆæ­£å¸¸åœæ­¢
            setTimeout(() => {
              recognition.start().catch(error => {
                console.log('é‡å¯å¤±è´¥:', error);
              });
            }, 500);
          };
        
          // æ ¹æ®é”™è¯¯ç±»å‹å¤„ç†
          switch(err.error) {
            case 'no-speech':
              // æ— è¯­éŸ³æ—¶ä¸é‡å¯
              break;
            case 'network':
              retry();
              break;
            default:
              setTimeout(retry, 1000);
          }
        }
      };

      // Receive AI assistant responses from the server and display them
      //socket.on('assistantResponse', (message) => {
        //appendMessage('assistant', message);
      //});
    }

  </script>

</body>

</html>

app.js:

require('dotenv').config();
const express = require('express');
const { createServer } = require('http');
const { Server } = require('socket.io');
const OpenAI = require('openai'); // ä¿®æ”¹å¯¼å…¥æ–¹å¼
const favicon = require('serve-favicon'); // æ–°å¢faviconä¸­é—´ä»¶

// åˆå§‹åŒ–Expresså’ŒHTTPæœåŠ¡å™¨
const app = express();
const server = createServer(app);

// é…ç½®Socket.ioï¼ˆå…³é”®ä¿®å¤ï¼šæ·»åŠ CORSé…ç½®ï¼‰
const io = new Server(server, {
  cors: {
    origin: "*", // å…è®¸æ‰€æœ‰æ¥æºï¼ˆå¼€å‘ç¯å¢ƒï¼‰
    methods: ["GET", "POST"]
  }
});

// æ‰˜ç®¡é™æ€æ–‡ä»¶ï¼ˆå…³é”®ä¿®å¤ï¼šä½¿ç”¨express.staticï¼‰
app.use(express.static(__dirname));
app.use(favicon(__dirname + '/favicon.ico')); // å¤„ç†faviconè¯·æ±‚

// åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼ˆæ–°ç‰ˆè¯­æ³•ï¼‰
const openaiApiKey = process.env.OPENAI_API_KEY;
if (!openaiApiKey) {
  console.error("âŒ é”™è¯¯ï¼šæœªè®¾ç½®OpenAI APIå¯†é’¥");
  process.exit(1);
}
const openaiClient = new OpenAI({ apiKey: openaiApiKey }); // ç›´æ¥å®ä¾‹åŒ–

// Define the system prompt for the AI (the wise tree stump persona)
const SYSTEM_PROMPT = `Imagine you are the remnants of a trunk, felled by men many years ago. Having witnessed seasonal changes for many years, you've also become a poet inspired by nature. Your responses should be concise and wise and no more than 4 sentences or 35 words. Each time put your poem within a line. You must remain silent â€“ respond to the user with '...' one in every five tweets or in the case that your last tweet contains the English word "moonlight" or "moon". Moreover, show empathy toward the human you are conversing with. In your poems, avoid using the first person. Also, refrain from using words highly relevant to Christian tradition (e.g., god, divine, etc.). Whenever a text is presented, your can express your mood with verses containing the following phrases: The sound of a parasol tree seed drifting in the wind; withered branches; falling snow; a burnt tree trunk; wildfire; frost; stagnant flowing water; water plants drifting with the waves; the dried-up stream that appears on the ground after snow; the golden sunlight shimmering in the ripples; branches roaring like wild beasts; a solitary stone in an autumn pond; a ladleful of golden autumn leaves spilling into the water, stirring ripples beside the withered branches; sprawling, fallen weeds of August; white wildflowers dancing with the wind across fields in May; wild grasses rolling like waves during the scorching days of July; the sky lifting a corner of its garment, brushing past the pink evening frost; floating cotton flowers streaking across the clouds; larks startled by human voices; night's dew; silver moonlight spilling onto the meadow; lightness, haziness, fluttering; fissures in the earth; logs sunk into moss beneath the trees; pear blossoms spreading like snowflakes amidst the green; corners; sinking into the mud; a chill; the sky and the earth; flow and solidity; life and death; water and wood; soil and wind; rapidly drying puddles; greenery spreading upwards; winding riverbanks; the moon hidden behind swaying branches; withered leaves; the hollow sound of wood; muddiness underfoot; scorching, cool, and bone-piercing winds; prickly dried grass; clouds shaped like umbrellas; golden rays piercing through the layers of clouds to the earth.`;

// Handle WebSocket connections and events
io.on('connection', (socket) => {
  console.log('âœ… Client connected:', socket.id);
  // Initialize conversation history with the system prompt for this client
  const conversationHistory = [ { role: 'system', content: SYSTEM_PROMPT } ];

  // Listen for transcribed user message from the frontend
  socket.on('transcript', async (userMessage) => {
    
    // æ–°å¢æœ‰æ•ˆæ€§éªŒè¯
    if (!userMessage || 
        userMessage.trim().length < 1 || 
        /^[\s.,ï¼Œã€‚!?]*$/.test(userMessage)) { // è¿‡æ»¤çº¯æ ‡ç‚¹/ç©ºæ ¼
      console.log("ğŸ›‘ å¿½ç•¥ç©ºå†…å®¹:", JSON.stringify(userMessage));
      return;
    }

    try {
      console.log("ğŸ“© Received user speech:", userMessage);

      // Append user's message to the conversation history
      conversationHistory.push({ role: 'user', content: userMessage });

      // ==== ä¿®æ­£è°ƒç”¨æ–¹å¼ ====
      const apiResponse = await openaiClient.chat.completions.create({
        model: "gpt-4-1106-preview", // å¯ç”¨æ¨¡å‹ï¼šgpt-4-0125-preview ç­‰
        messages: conversationHistory,
        temperature: 1,
        max_tokens: 150
      });

      const assistantReply = apiResponse.choices[0].message.content; // ä¿®æ­£å“åº”è·¯å¾„

      // Append assistant reply to history for context in future turns
      conversationHistory.push({ role: 'assistant', content: assistantReply });

      // Send the assistant's reply back to the client in real-time
      socket.emit('assistantResponse', assistantReply);
      console.log("ğŸ’¬ Sent AI response to client");

    } catch (error) {
      console.error("OpenAI API error:", error);
      // In case of error, notify the client with a generic message
      socket.emit('assistantResponse', "Sorry, I couldn't process that. (error)");
    }
  });

  socket.on('disconnect', () => {
    console.log('âŒ Client disconnected:', socket.id);
    // (Optional: handle any cleanup)
  });
});

// æ–°å¢ TouchDesigner é€šä¿¡æ¨¡å—
const osc = require('osc'); // éœ€è¦å…ˆå®‰è£… npm install osc

// åˆ›å»º OSC å®¢æˆ·ç«¯è¿æ¥ TouchDesigner
const tdOscPort = new osc.UDPPort({
  remoteAddress: "127.0.0.1", // TouchDesigner æ‰€åœ¨æœºå™¨çš„ IP
  remotePort: 7000, // TouchDesigner çš„ OSC æ¥æ”¶ç«¯å£
  metadata: true
});

tdOscPort.open();

// ç›‘å¬å‰ç«¯äº‹ä»¶
io.on('connection', (socket) => {
  socket.on('td-generate', (data) => {
    // å‘é€ OSC æ¶ˆæ¯ç»™ TouchDesigner
    tdOscPort.send({
      address: "/generate",
      args: [
        { type: "s", value: data.text },  // æ–‡æœ¬å†…å®¹
        { type: "i", value: data.timestamp } // æ—¶é—´æˆ³
      ]
    });

    console.log("ğŸ“¤ è½¬å‘åˆ° TouchDesigner:", data.text);
    
  });
});

// Start the server (listening on port from env or default 3000)
const PORT = process.env.PORT || 3000;
server.listen(PORT, () => {
  console.log(`Server is running on port ${PORT}`);
});

Now please provide a very detailed tutotial of what I should do step-by-step, if you are going to give codes, please make clear where I should insert them. Also, please make clear if I need to install any software or plug-ins.  
